//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_75
.address_size 64

	// .globl	wmmaGemmF16

.visible .entry wmmaGemmF16(
	.param .u64 wmmaGemmF16_param_0,
	.param .u64 wmmaGemmF16_param_1,
	.param .u64 wmmaGemmF16_param_2,
	.param .u64 wmmaGemmF16_param_3,
	.param .u32 wmmaGemmF16_param_4,
	.param .u32 wmmaGemmF16_param_5,
	.param .u32 wmmaGemmF16_param_6
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<161>;
	.reg .b32 	%r<126>;
	.reg .b64 	%rd<34>;


	ld.param.u64 	%rd6, [wmmaGemmF16_param_0];
	ld.param.u64 	%rd7, [wmmaGemmF16_param_1];
	ld.param.u64 	%rd4, [wmmaGemmF16_param_2];
	ld.param.u64 	%rd5, [wmmaGemmF16_param_3];
	ld.param.u32 	%r23, [wmmaGemmF16_param_4];
	ld.param.u32 	%r21, [wmmaGemmF16_param_5];
	ld.param.u32 	%r22, [wmmaGemmF16_param_6];
	cvta.to.global.u64 	%rd1, %rd7;
	cvta.to.global.u64 	%rd2, %rd6;
	mov.u32 	%r24, %ctaid.x;
	shl.b32 	%r1, %r24, 4;
	mov.u32 	%r25, %ctaid.y;
	shl.b32 	%r2, %r25, 4;
	setp.ge.u32 	%p1, %r2, %r23;
	setp.ge.u32 	%p2, %r1, %r21;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB0_9;

	cvta.to.global.u64 	%rd8, %rd4;
	mad.lo.s32 	%r26, %r2, %r21, %r1;
	cvt.u64.u32 	%rd3, %r26;
	mul.wide.u32 	%rd9, %r26, 4;
	add.s64 	%rd10, %rd8, %rd9;
	wmma.load.c.sync.aligned.row.m16n16k16.global.f32 	{%f129, %f130, %f131, %f132, %f133, %f134, %f135, %f136}, [%rd10], %r21;
	setp.lt.s32 	%p4, %r22, 1;
	@%p4 bra 	$L__BB0_8;

	mul.lo.s32 	%r3, %r2, %r22;
	add.s32 	%r28, %r22, -1;
	shr.u32 	%r29, %r28, 4;
	add.s32 	%r4, %r29, 1;
	and.b32  	%r125, %r4, 3;
	setp.lt.u32 	%p5, %r28, 48;
	mov.u32 	%r122, 0;
	@%p5 bra 	$L__BB0_5;

	sub.s32 	%r121, %r4, %r125;
	mov.u32 	%r122, 0;

$L__BB0_4:
	add.s32 	%r31, %r122, %r3;
	mul.wide.u32 	%rd11, %r31, 2;
	add.s64 	%rd12, %rd2, %rd11;
	mad.lo.s32 	%r32, %r122, %r21, %r1;
	mul.wide.u32 	%rd13, %r32, 2;
	add.s64 	%rd14, %rd1, %rd13;
	wmma.load.a.sync.aligned.row.m16n16k16.global.f16 	{%r33, %r34, %r35, %r36, %r37, %r38, %r39, %r40}, [%rd12], %r22;
	wmma.load.b.sync.aligned.row.m16n16k16.global.f16 	{%r41, %r42, %r43, %r44, %r45, %r46, %r47, %r48}, [%rd14], %r21;
	wmma.mma.sync.aligned.row.row.m16n16k16.f32.f32 {%f97, %f98, %f99, %f100, %f101, %f102, %f103, %f104}, {%r33, %r34, %r35, %r36, %r37, %r38, %r39, %r40}, {%r41, %r42, %r43, %r44, %r45, %r46, %r47, %r48}, {%f129, %f130, %f131, %f132, %f133, %f134, %f135, %f136};
	add.s32 	%r49, %r31, 16;
	mul.wide.u32 	%rd15, %r49, 2;
	add.s64 	%rd16, %rd2, %rd15;
	shl.b32 	%r50, %r21, 4;
	add.s32 	%r51, %r32, %r50;
	mul.wide.u32 	%rd17, %r51, 2;
	add.s64 	%rd18, %rd1, %rd17;
	wmma.load.a.sync.aligned.row.m16n16k16.global.f16 	{%r52, %r53, %r54, %r55, %r56, %r57, %r58, %r59}, [%rd16], %r22;
	wmma.load.b.sync.aligned.row.m16n16k16.global.f16 	{%r60, %r61, %r62, %r63, %r64, %r65, %r66, %r67}, [%rd18], %r21;
	wmma.mma.sync.aligned.row.row.m16n16k16.f32.f32 {%f105, %f106, %f107, %f108, %f109, %f110, %f111, %f112}, {%r52, %r53, %r54, %r55, %r56, %r57, %r58, %r59}, {%r60, %r61, %r62, %r63, %r64, %r65, %r66, %r67}, {%f97, %f98, %f99, %f100, %f101, %f102, %f103, %f104};
	add.s32 	%r68, %r31, 32;
	mul.wide.u32 	%rd19, %r68, 2;
	add.s64 	%rd20, %rd2, %rd19;
	add.s32 	%r69, %r51, %r50;
	mul.wide.u32 	%rd21, %r69, 2;
	add.s64 	%rd22, %rd1, %rd21;
	wmma.load.a.sync.aligned.row.m16n16k16.global.f16 	{%r70, %r71, %r72, %r73, %r74, %r75, %r76, %r77}, [%rd20], %r22;
	wmma.load.b.sync.aligned.row.m16n16k16.global.f16 	{%r78, %r79, %r80, %r81, %r82, %r83, %r84, %r85}, [%rd22], %r21;
	wmma.mma.sync.aligned.row.row.m16n16k16.f32.f32 {%f113, %f114, %f115, %f116, %f117, %f118, %f119, %f120}, {%r70, %r71, %r72, %r73, %r74, %r75, %r76, %r77}, {%r78, %r79, %r80, %r81, %r82, %r83, %r84, %r85}, {%f105, %f106, %f107, %f108, %f109, %f110, %f111, %f112};
	add.s32 	%r86, %r31, 48;
	mul.wide.u32 	%rd23, %r86, 2;
	add.s64 	%rd24, %rd2, %rd23;
	add.s32 	%r87, %r69, %r50;
	mul.wide.u32 	%rd25, %r87, 2;
	add.s64 	%rd26, %rd1, %rd25;
	wmma.load.a.sync.aligned.row.m16n16k16.global.f16 	{%r88, %r89, %r90, %r91, %r92, %r93, %r94, %r95}, [%rd24], %r22;
	wmma.load.b.sync.aligned.row.m16n16k16.global.f16 	{%r96, %r97, %r98, %r99, %r100, %r101, %r102, %r103}, [%rd26], %r21;
	wmma.mma.sync.aligned.row.row.m16n16k16.f32.f32 {%f129, %f130, %f131, %f132, %f133, %f134, %f135, %f136}, {%r88, %r89, %r90, %r91, %r92, %r93, %r94, %r95}, {%r96, %r97, %r98, %r99, %r100, %r101, %r102, %r103}, {%f113, %f114, %f115, %f116, %f117, %f118, %f119, %f120};
	add.s32 	%r122, %r122, 64;
	add.s32 	%r121, %r121, -4;
	setp.ne.s32 	%p6, %r121, 0;
	@%p6 bra 	$L__BB0_4;

$L__BB0_5:
	setp.eq.s32 	%p7, %r125, 0;
	@%p7 bra 	$L__BB0_8;

	mad.lo.s32 	%r124, %r122, %r21, %r1;
	shl.b32 	%r13, %r21, 4;
	add.s32 	%r123, %r122, %r3;

$L__BB0_7:
	.pragma "nounroll";
	mul.wide.u32 	%rd27, %r123, 2;
	add.s64 	%rd28, %rd2, %rd27;
	mul.wide.u32 	%rd29, %r124, 2;
	add.s64 	%rd30, %rd1, %rd29;
	wmma.load.a.sync.aligned.row.m16n16k16.global.f16 	{%r104, %r105, %r106, %r107, %r108, %r109, %r110, %r111}, [%rd28], %r22;
	wmma.load.b.sync.aligned.row.m16n16k16.global.f16 	{%r112, %r113, %r114, %r115, %r116, %r117, %r118, %r119}, [%rd30], %r21;
	wmma.mma.sync.aligned.row.row.m16n16k16.f32.f32 {%f129, %f130, %f131, %f132, %f133, %f134, %f135, %f136}, {%r104, %r105, %r106, %r107, %r108, %r109, %r110, %r111}, {%r112, %r113, %r114, %r115, %r116, %r117, %r118, %r119}, {%f129, %f130, %f131, %f132, %f133, %f134, %f135, %f136};
	add.s32 	%r124, %r124, %r13;
	add.s32 	%r123, %r123, 16;
	add.s32 	%r125, %r125, -1;
	setp.eq.s32 	%p8, %r125, 0;
	@%p8 bra 	$L__BB0_8;
	bra.uni 	$L__BB0_7;

$L__BB0_8:
	cvta.to.global.u64 	%rd31, %rd5;
	shl.b64 	%rd32, %rd3, 2;
	add.s64 	%rd33, %rd31, %rd32;
	wmma.store.d.sync.aligned.row.m16n16k16.global.f32 	[%rd33], {%f129, %f130, %f131, %f132, %f133, %f134, %f135, %f136}, %r21;

$L__BB0_9:
	ret;

}

